{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83540ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with Ollama\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# langchain and community/partner packages\n",
    "import pinecone\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama  # <-- IMPORT OLLAMA\n",
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62bb77af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables.\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables (only need Pinecone key now)\n",
    "load_dotenv()\n",
    "print(\"Loaded environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d9aa812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(directory):\n",
    "    file_loader = PyPDFDirectoryLoader(directory)\n",
    "    return file_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58220a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(docs, chunk_size=800, chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eebdffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "708c473e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading and chunking documents...\n",
      "Loaded and chunked 9 document segments.\n"
     ]
    }
   ],
   "source": [
    "# Document loading and processing (remains the same)\n",
    "print(\"\\nReading and chunking documents...\")\n",
    "raw_docs = read_doc(\"documents\")\n",
    "documents = chunk_data(docs=raw_docs)\n",
    "print(f\"Loaded and chunked {len(documents)} document segments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fc764e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing embeddings model...\n",
      "Embeddings initialized. Vector dimension: 768\n"
     ]
    }
   ],
   "source": [
    " # Embeddings model (remains the same)\n",
    "print(\"\\nInitializing embeddings model...\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vector_test = embeddings.embed_query(\"This is a test.\")\n",
    "print(f\"Embeddings initialized. Vector dimension: {len(vector_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29c09ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up Pinecone vector store...\n",
      "Index 'langchainvector' already exists.\n",
      "Success! Vector store is ready.\n"
     ]
    }
   ],
   "source": [
    "# Pinecone setup (remains the same)\n",
    "print(\"\\nSetting up Pinecone vector store...\")\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "if not api_key: raise ValueError(\"PINECONE_API_KEY not found\")\n",
    "pc = pinecone.Pinecone(api_key=api_key)\n",
    "index_name = \"langchainvector\"\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    print(f\"Creating index '{index_name}'...\")\n",
    "    pc.create_index(name=index_name, dimension=len(vector_test), metric=\"cosine\", spec=pinecone.ServerlessSpec(cloud=\"aws\", region=\"us-west-2\"))\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists.\")\n",
    "vectorstore = PineconeVectorStore.from_documents(documents, embeddings, index_name=index_name)\n",
    "print(\"Success! Vector store is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f21fb08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building the RAG chain using a local Ollama model...\n",
      "Retriever and RAG chain created successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup RAG Chain with Ollama ---\n",
    "print(\"\\nBuilding the RAG chain using a local Ollama model...\")\n",
    "    \n",
    "# 1. Retriever (remains the same)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 2})\n",
    "\n",
    "# 2. Initialize the LLM using Ollama\n",
    "# Make sure you have pulled this model with `ollama pull llama3:8b-instruct-q4_K_M`\n",
    "llm = ChatOllama(model=\"llama3:8b-instruct-q4_K_M\", temperature=0.7)\n",
    "\n",
    "# 3. Prompt Template (remains the same)\n",
    "template = \"Use the following pieces of context to answer the question. If you don't know the answer, just say that you don't know.\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nHelpful Answer:\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 4. RAG Chain (remains the same)\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"Retriever and RAG chain created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67d55b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Invoking RAG chain with query: 'What are the skills' ---\n",
      "\n",
      "--- Question ---\n",
      "What are the skills\n",
      "\n",
      "--- Answer ---\n",
      "Based on the provided context, here are the skills mentioned:\n",
      "\n",
      "**Data Science & Machine Learning**\n",
      "\n",
      "* Deep Learning (CNN, RNN, Transfer Learning)\n",
      "* Machine Learning\n",
      "* Data Science\n",
      "* Data Visualization\n",
      "* SQL\n",
      "\n",
      "**Computer Science Basics**\n",
      "\n",
      "* Data Structures & Algorithms (DSA)\n",
      "* Linux Commands\n",
      "* Operating System (OS)\n",
      "\n",
      "**Web Development**\n",
      "\n",
      "* HTML\n",
      "* CSS\n",
      "\n",
      "**MERN Stack**\n",
      "\n",
      "* MongoDB\n",
      "* ExpressJS\n",
      "* React\n",
      "* NodeJS\n",
      "\n",
      "**Version Control**\n",
      "\n",
      "* Git\n",
      "* GitHub\n",
      "\n",
      "**Soft Skills**\n",
      "\n",
      "* Presentation\n",
      "* Teamwork & Collaboration\n",
      "* Time Management\n",
      "* Adaptability\n",
      "* Problem Solving\n",
      "\n",
      "**Frameworks & Libraries**\n",
      "\n",
      "* TensorFlow\n",
      "* Keras\n",
      "* PyTorch\n",
      "* Scikit-learn\n",
      "* Pandas\n",
      "* NumPy\n",
      "* Matplotlib\n",
      "\n",
      "Additionally, the context mentions that a comprehensive project was developed for implementing and simulating various CPU scheduling algorithms, demonstrating a foundational understanding of operating system principles.\n"
     ]
    }
   ],
   "source": [
    "# --- Invoke the Chain ---\n",
    "our_query = \"What are the skills\"\n",
    "print(f\"\\n--- Invoking RAG chain with query: '{our_query}' ---\")\n",
    "answer = rag_chain.invoke(our_query)\n",
    "print(\"\\n--- Question ---\\n\" + our_query)\n",
    "print(\"\\n--- Answer ---\\n\" + answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b3bafb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
